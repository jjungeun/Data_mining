{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    if os.path.exists(filename) is False:\n",
    "        with open(filename,\"wb\") as file:\n",
    "            res = requests.get(url)\n",
    "            file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_file(pdfFile):\n",
    "    logging.propagate = False \n",
    "    logging.getLogger().setLevel(logging.ERROR)\n",
    "    with open(pdfFile, mode='rb') as file:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "        process_pdf(rsrcmgr, device, file)\n",
    "        device.close()\n",
    "\n",
    "        content = retstr.getvalue()\n",
    "        retstr.close()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec1(page):\n",
    "    res = requests.get('http://www.kats.go.kr/content.do?cmsid=304&page=' + str(page)).text\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    crawl_ls = soup.select('table[summary=\"제품안전 전기용품리스트로 , 기준번호 , 기준명 , 고시일 , 조회수 , 첨부파일  제공\"] tbody tr')\n",
    "    \n",
    "    return crawl_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent_sec1(crawl_ls, fp):\n",
    "    for crawl in crawl_ls:\n",
    "        file_content = ''\n",
    "        std_num = crawl.select('td')[0].text\n",
    "        std_name = crawl.select('td')[1].text\n",
    "        date = crawl.select('td')[2].text\n",
    "        \n",
    "        # 첨부파일 1개라고 가정\n",
    "        crawl_file = crawl.select('td[class=\"no_b_right\"] a')[0]\n",
    "        file_name = crawl_file.select('img')[0].get('title')\n",
    "        file_path = 'http://www.kats.go.kr' + crawl_file.get('href')\n",
    "\n",
    "        if 'pdf' in file_name:\n",
    "            download(file_path, file_name)\n",
    "            file_content = read_pdf_file(file_name).replace('\\n','')\n",
    "\n",
    "        # 파일에 저장 TODO: ','무시하기\n",
    "        if file_content is not '':\n",
    "            fp.write('\"{0}\",\"{1}\",\"{2}\",\"{3}\",\"{4}\"\\n'.format(std_num, std_name, date, file_name, file_content.strip()))\n",
    "            \n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('sec1.csv', 'wt', encoding = 'utf-8')\n",
    "for page in range(1,2):\n",
    "    url_ls = sec1(page)\n",
    "    fp = getContent_sec1(url_ls, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec2(page):\n",
    "    url_ls = []\n",
    "    res = requests.get('http://www.kats.go.kr/content.do?cmsid=527&page='+str(page)).text\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    crawl_ls = soup.select('table[summary=\"생활용품 안전기준리스트로 , 구분 , 분야 , 제목 , 작성자 , 작성일 , 조회수  제공\"] tbody tr .tbleft a')\n",
    "    for crawl in crawl_ls:\n",
    "        url_ls.append(crawl.get('href'))\n",
    "    return url_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec3(page):\n",
    "    url_ls = []\n",
    "    res = requests.get('http://www.kats.go.kr/content.do?cmsid=530&page='+str(page)).text\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    crawl_ls = soup.select('table[summary=\"어린이제품 안전기준리스트로 , 구분 , 제목 , 작성자 , 작성일 , 조회수  제공\"] tbody tr .tbleft a')\n",
    "    for crawl in crawl_ls:\n",
    "        url_ls.append(crawl.get('href'))\n",
    "    return url_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sec2, sec3는 기준번호, 고시일이 없음(우선 구분, 분야, 작성일로 대체)\n",
    "def getContent(url_ls, fp):\n",
    "    for url in url_ls:\n",
    "        file_content = ''\n",
    "        res = requests.get('http://www.kats.go.kr' + url).text\n",
    "        soup = BeautifulSoup(res,'html.parser')\n",
    "        \n",
    "        crawl = soup.select('table')[0]\n",
    "        std_name = crawl.select('thead th[class=\"no_b_right tbleft\"]')[0].text\n",
    "        date = crawl.select('tbody td[class=\"no_b_right\"]')[0].text\n",
    "        crawl_body = crawl.select('tbody td[class=\"no_b_right tbleft\"]')\n",
    "        div = crawl_body[0].text\n",
    "        field = crawl_body[1].text\n",
    "        \n",
    "        for crawl_file in crawl_body[2].select('a'):\n",
    "            file_name = crawl_file.text\n",
    "            if 'pdf' in file_name:\n",
    "                file_path = 'http://www.kats.go.kr' + crawl_file.get('href')\n",
    "                download(file_path, file_name)\n",
    "                file_content = read_pdf_file(file_name).replace('\\n','')\n",
    "            elif len(crawl_file) == 1 and 'hwp' in file_name:\n",
    "                pass\n",
    "#                 download(file_path, file_name) # hwp 파일만 있는 경우\n",
    "        \n",
    "        if file_content is not '':\n",
    "            fp.write('\"{0}\",\"{1}\",\"{2}\",\"{3}\",\"{4}\",\"{5}\"\\n'.format(div, field, std_name, date, file_name, file_content.strip()))\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('sec2.csv', 'wt', encoding = 'utf-8')\n",
    "for page in range(1,2):\n",
    "    url_ls = sec2(page)\n",
    "    fp = getContent(url_ls, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('sec3.csv', 'wt', encoding = 'utf-8')\n",
    "for page in range(1,2):\n",
    "    url_ls = sec3(page)\n",
    "    fp = getContent(url_ls, fp)\n",
    "fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
